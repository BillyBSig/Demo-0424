{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 d:\\YoloDemo\\meeting.jpg: 448x640 7 persons, 1 chair, 1 potted plant, 2 laptops, 49.0ms\n",
      "Speed: 5.0ms preprocess, 49.0ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mcoba\\aa\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "import os\n",
    "results = model(\"meeting.jpg\", task=\"detect\",save=True, conf=0.5, project=\"coba\", name=\"aa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yolo task=detect mode=predict model=yolov8n.pt source=marathon.mp4 conf=0.5 show=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i {\"runs\\detect\\predict3\\marathon.avi\"} -vcodec libx264 {r\"runs\\detect\\predict3\\final.mp4\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def predict_process(model_path, target):\n",
    "    predict = dt.now().strftime('%Y-%m-%d-%H%M%S')\n",
    "    model = YOLO(model_path)\n",
    "    results = model(target, task=\"detect\", save=True, conf=0.5, project=\"detect\", name=predict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.2  Python-3.10.0 torch-2.2.1+cu121 CPU (12th Gen Intel Core(TM) i7-12700H)\n",
      "YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'model\\yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
      "Collecting onnx>=1.12.0\n",
      "  Downloading onnx-1.16.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\billy\\miniconda3\\envs\\yolo_demo\\lib\\site-packages (from onnx>=1.12.0) (1.26.3)\n",
      "Collecting protobuf>=3.20.2 (from onnx>=1.12.0)\n",
      "  Downloading protobuf-5.26.1-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Downloading onnx-1.16.0-cp310-cp310-win_amd64.whl (14.4 MB)\n",
      "   ---------------------------------------- 14.4/14.4 MB 9.0 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.26.1-cp310-abi3-win_amd64.whl (420 kB)\n",
      "   ---------------------------------------- 420.9/420.9 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf, onnx\n",
      "Successfully installed onnx-1.16.0 protobuf-5.26.1\n",
      "\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success  13.4s, installed 1 package: ['onnx>=1.12.0']\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  14.8s, saved as 'model\\yolov8n.onnx' (12.2 MB)\n",
      "\n",
      "Export complete (18.1s)\n",
      "Results saved to \u001b[1mD:\\YoloDemo\\model\u001b[0m\n",
      "Predict:         yolo predict task=detect model=model\\yolov8n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=model\\yolov8n.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model\\\\yolov8n.onnx'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"model\\yolov8n.pt\")\n",
    "\n",
    "model.export(format=\"onnx\", imgsz=[640,640], opset=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m x_factor \u001b[38;5;241m=\u001b[39m image_width \u001b[38;5;241m/\u001b[39m INPUT_WIDTH\n\u001b[0;32m     21\u001b[0m y_factor \u001b[38;5;241m=\u001b[39m image_height \u001b[38;5;241m/\u001b[39m INPUT_HEIGHT\n\u001b[1;32m---> 23\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mpreds\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rows):\n\u001b[0;32m     26\u001b[0m     row \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m0\u001b[39m][i]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "net = cv2.dnn.readNetFromONNX(\"model\\yolov8n.onnx\")\n",
    "INPUT_WIDTH = 640\n",
    "INPUT_HEIGHT = 640\n",
    "\n",
    "image = cv2.imread('meeting.jpg')\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255.0, (INPUT_WIDTH, INPUT_HEIGHT), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "output = net.forward()\n",
    "output = output.transpose((0, 2, 1))\n",
    "\n",
    "\n",
    "# Extract output detection\n",
    "class_ids, confs, boxes = list(), list(), list()\n",
    "\n",
    "image_height, image_width, _ = image.shape\n",
    "x_factor = image_width / INPUT_WIDTH\n",
    "y_factor = image_height / INPUT_HEIGHT\n",
    "\n",
    "rows = preds[0].shape[0]\n",
    "\n",
    "for i in range(rows):\n",
    "    row = preds[0][i]\n",
    "    conf = row[4]\n",
    "    \n",
    "    classes_score = row[4:]\n",
    "    _,_,_, max_idx = cv2.minMaxLoc(classes_score)\n",
    "    class_id = max_idx[1]\n",
    "    if (classes_score[class_id] > .25):\n",
    "        confs.append(conf)\n",
    "        label = int(class_id)\n",
    "        class_ids.append(label)\n",
    "        \n",
    "        #extract boxes\n",
    "        x, y, w, h = row[0].item(), row[1].item(), row[2].item(), row[3].item() \n",
    "        left = int((x - 0.5 * w) * x_factor)\n",
    "        top = int((y - 0.5 * h) * y_factor)\n",
    "        width = int(w * x_factor)\n",
    "        height = int(h * y_factor)\n",
    "        box = np.array([left, top, width, height])\n",
    "        boxes.append(box)\n",
    "        \n",
    "r_class_ids, r_confs, r_boxes = list(), list(), list()\n",
    "\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confs, 0.25, 0.45) \n",
    "for i in indexes:\n",
    "    r_class_ids.append(class_ids[i])\n",
    "    r_confs.append(confs[i])\n",
    "    r_boxes.append(boxes[i])\n",
    "\n",
    "\n",
    "\n",
    "for i in indexes:\n",
    "    box = boxes[i]\n",
    "    left = box[0]\n",
    "    top = box[1]\n",
    "    width = box[2]\n",
    "    height = box[3]\n",
    "    conf = confs[i]\n",
    "    \n",
    "    cv2.rectangle(image, (left, top), (left + width, top + height), (0,255,0), 3)\n",
    "\n",
    "cv2.imwrite(\"your_result_image.jpg\", image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading model\\yolov8n.onnx for ONNX Runtime inference...\n",
      "\n",
      "image 1/1 d:\\YoloDemo\\meeting.jpg: 640x640 7 persons, 1 chair, 1 potted plant, 2 laptops, 1 vase, 172.4ms\n",
      "Speed: 3.0ms preprocess, 172.4ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mcoba\\aa8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load the exported ONNX model\n",
    "onnx_model = YOLO('model\\yolov8n.onnx')\n",
    "\n",
    "# Run inference\n",
    "results = onnx_model('meeting.jpg', task=\"detect\",save=True, conf=0.5, project=\"coba\", name=\"aa\", device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
